# pyimgano

**Enterprise-Grade Visual Anomaly Detection Toolkit**

[![Python](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

A comprehensive, production-ready Python toolkit for visual anomaly detection, integrating **120+ registered model entry points** (native implementations + optional backend wrappers + legacy aliases) from classical machine learning to cutting-edge deep learning (**CVPR 2025**, **CVPR 2024**, **ECCV 2024**, CVPR 2023, ICCV 2023, CVPR 2022, ECCV 2020).

> **Translations:** [ä¸­æ–‡](README_cn.md) Â· [æ—¥æœ¬èª](README_ja.md) Â· [í•œêµ­ì–´](README_ko.md)

---

## âœ¨ Key Features

- ğŸ”¥ **120+ Registry Model Entries** - From classical (ECOD, COPOD, KNN, PCA) to recent SOTA methods (including optional backend checkpoint wrappers)
- ğŸš€ **Production Ready** - Enterprise-grade code quality, comprehensive testing, CI/CD pipelines
- ğŸ“¦ **Unified API** - Consistent interface across all algorithms with factory pattern
- âš¡ **High Performance** - Top-tier algorithms (ECOD, COPOD) optimized for speed and accuracy
- ğŸ¯ **Flexible** - Works with any feature extractor or end-to-end deep learning
- ğŸ­ **Industrial Inference (numpy-first)** â­ NEW! - Explicit `ImageFormat`, canonical `RGB/u8/HWC` inputs, JSONL `pyimgano-infer` CLI
- ğŸ§© **High-Resolution Tiling Inference** â­ NEW! - Tiled scoring + anomaly-map stitching for 2K/4K inspection images
- ğŸ–¼ï¸ **Image Preprocessing** - 80+ operations (edge detection, morphology, filters, FFT, texture analysis, segmentation, augmentation)
- ğŸ“Š **Dataset Loaders** â­ NEW! - MVTec AD, MVTec LOCO AD, MVTec AD 2, VisA, BTAD, custom datasets with automatic loading
- ğŸ“ˆ **Advanced Visualization** â­ NEW! - ROC/PR curves, confusion matrices, t-SNE, anomaly heatmaps
- ğŸ§ª **Pixel-Level Metrics** â­ NEW! - pixel AUROC/AP + **region-aware AUPRO** (FPR-limited integration) for industrial inspection
- ğŸ’¾ **Model Management** â­ NEW! - Save/load, versioning, profiling, model registry
- ğŸ”¬ **Experiment Tracking** â­ NEW! - Hyperparameter logging, metric tracking, report generation
- ğŸ† **Built-in Benchmarking** - Compare multiple algorithms systematically
- ğŸ“– **Well Documented** - Extensive docs, algorithm guide, and examples
- ğŸ”§ **Easy to Extend** - Plugin architecture with model registry system
- ğŸ§¾ **Auto-generated Model Index** â­ NEW! - See `docs/MODEL_INDEX.md` or run `pyimgano-benchmark --list-models`

---

## ğŸ† Highlights

### State-of-the-Art Algorithms

| Algorithm | Type | Year | Performance | Speed | Use Case |
|-----------|------|------|-------------|-------|----------|
| **One-for-More** â­ ğŸš€ | Deep Learning | 2025 | â­â­â­â­â­ | âš¡âš¡âš¡ | #1 on MVTec/VisA, continual learning |
| **BayesianPF** â­ ğŸš€ | Deep Learning | 2025 | â­â­â­â­ | âš¡âš¡âš¡ | Zero-shot Bayesian inference |
| **Odd-One-Out** â­ ğŸš€ | Deep Learning | 2025 | â­â­â­â­ | âš¡âš¡âš¡âš¡ | Neighbor comparison, fast |
| **CrossMAD** â­ ğŸš€ | Deep Learning | 2025 | â­â­â­â­ | âš¡âš¡âš¡ | Cross-modal harmonization |
| **InCTRL** â­ ğŸ”¥ | Deep Learning | 2024 | â­â­â­â­â­ | âš¡âš¡âš¡ | In-context learning, few-shot generalist |
| **RealNet** â­ ğŸ”¥ | Deep Learning | 2024 | â­â­â­â­â­ | âš¡âš¡âš¡ | Feature selection, realistic synthesis |
| **PromptAD** â­ ğŸ”¥ | Deep Learning | 2024 | â­â­â­â­ | âš¡âš¡âš¡ | Prompt learning, few-shot AD |
| **GLAD** â­ ğŸ”¥ | Deep Learning | 2024 | â­â­â­â­â­ | âš¡âš¡ | Adaptive diffusion, reconstruction |
| **AST** â­ ğŸ†• | Deep Learning | 2023 | â­â­â­â­â­ | âš¡âš¡âš¡ | Anomaly-aware training, robust |
| **DST** â­ ğŸ†• | Deep Learning | 2023 | â­â­â­â­â­ | âš¡âš¡âš¡ | Double student-teacher, complementary |
| **PANDA** â­ ğŸ†• | Deep Learning | 2023 | â­â­â­â­ | âš¡âš¡âš¡ | Prototypical learning, metric-based |
| **RegAD** â­ ğŸ†• | Deep Learning | 2023 | â­â­â­â­ | âš¡âš¡âš¡ | Registration-based, alignment |
| **GCAD** â­ ğŸ†• | Deep Learning | 2023 | â­â­â­â­ | âš¡âš¡ | Graph convolution, spatial relations |
| **FAVAE** â­ ğŸ†• | Deep Learning | 2023 | â­â­â­â­ | âš¡âš¡âš¡ | Feature adaptive VAE, dynamic |
| **InTra** â­ | Deep Learning | 2023 | â­â­â­â­â­ | âš¡âš¡âš¡ | Transformer-based, long-range |
| **WinCLIP** â­ | Deep Learning | 2023 | â­â­â­â­â­ | âš¡âš¡ | Zero-shot, no training needed |
| **SimpleNet** â­ | Deep Learning | 2023 | â­â­â­â­â­ | âš¡âš¡âš¡âš¡ | Ultra-fast SOTA, production |
| **BGAD** â­ | Deep Learning | 2023 | â­â­â­â­ | âš¡âš¡âš¡ | Background-guided, robust |
| **DifferNet** â­ | Deep Learning | 2023 | â­â­â­â­ | âš¡âš¡âš¡ | Learnable differences, k-NN |
| **DSR** â­ | Deep Learning | 2023 | â­â­â­â­ | âš¡âš¡âš¡âš¡ | Frequency domain, parameter-free |
| **PatchCore** â­ | Deep Learning | 2022 | â­â­â­â­â­ | âš¡âš¡âš¡ | Best accuracy, MVTec champion |
| **PNI** â­ | Deep Learning | 2022 | â­â­â­â­â­ | âš¡âš¡âš¡ | Multi-scale pyramid, fast |
| **RD++** â­ | Deep Learning | 2022 | â­â­â­â­ | âš¡âš¡âš¡ | Enhanced distillation, attention |
| **ECOD** | Classical | 2022 | â­â­â­â­â­ | âš¡âš¡âš¡ | Parameter-free, general purpose |
| **COPOD** | Classical | 2020 | â­â­â­â­â­ | âš¡âš¡âš¡âš¡ | Speed-critical applications |
| **SPADE** â­ | Deep Learning | 2020 | â­â­â­â­â­ | âš¡âš¡âš¡ | Excellent localization, k-NN |
| **CSFlow** | Deep Learning | 2022 | â­â­â­â­ | âš¡âš¡âš¡ | Expressive flows, good accuracy |
| **CutPaste** | Deep Learning | 2021 | â­â­â­â­ | âš¡âš¡âš¡ | Self-supervised, no anomalies |
| **STFPM** | Deep Learning | 2021 | â­â­â­â­ | âš¡âš¡ | Student-Teacher, localization |

> **See [Algorithm Selection Guide](docs/ALGORITHM_SELECTION_GUIDE.md) and [Deep Learning Guide](docs/DEEP_LEARNING_MODELS.md) for detailed comparison**

---

## ğŸ“¦ Installation

### Basic Installation

```bash
pip install pyimgano
```

> Note: `pip install pyimgano` works after publishing to PyPI. If you are
> installing from source, use the development installation below. For the
> release process, see `docs/PUBLISHING.md`.

### Development Installation

```bash
git clone https://github.com/skygazer42/pyimgano.git
cd pyimgano
pip install -e .[dev]
```

### With Optional Dependencies

```bash
# For diffusion models
pip install pyimgano[diffusion]

# For advanced visualization (seaborn)
pip install pyimgano[viz]

# For anomalib checkpoint wrappers (inference-first)
pip install pyimgano[anomalib]

# For FAISS kNN acceleration
pip install pyimgano[faiss]

# For OpenCLIP backends
pip install "pyimgano[clip]"

# For MambaAD (Mamba SSM sequence model)
pip install "pyimgano[mamba]"

# All optional backends
pip install pyimgano[backends]

# For documentation
pip install pyimgano[docs]

# Everything
pip install pyimgano[all]
```

### Requirements

- Python >= 3.9
- PyTorch >= 1.9.0
- PyOD >= 1.1.0
- scikit-learn >= 0.22.0

---

## ğŸš€ Quick Start

### Example 1: Using ECOD (Recommended for First-Time Users)

```python
from pyimgano import models, utils

# 1. Create feature extractor
feature_extractor = utils.ImagePreprocessor(
    resize=(224, 224),
    output_tensor=False
)

# 2. Create detector (ECOD: parameter-free, top performance)
detector = models.create_model(
    "vision_ecod",
    feature_extractor=feature_extractor,
    contamination=0.1,  # Expected anomaly ratio
    n_jobs=-1  # Use all CPU cores
)

# 3. Train on normal images
train_paths = ["normal_1.jpg", "normal_2.jpg", ...]
detector.fit(train_paths)

# 4. Detect anomalies
test_paths = ["test_1.jpg", "test_2.jpg", ...]
predictions = detector.predict(test_paths)  # 0: normal, 1: anomaly
scores = detector.decision_function(test_paths)  # Anomaly scores

print(f"Detected {predictions.sum()} anomalies")
```

### Example 1.5: Industrial Inference (numpy-first, explicit ImageFormat)

If you already have decoded frames in memory (e.g. video pipelines), use the
numpy-first helpers in `pyimgano.inputs` + `pyimgano.inference`.

See: `docs/INDUSTRIAL_INFERENCE.md`

```python
import numpy as np

from pyimgano.inference import calibrate_threshold, infer
from pyimgano.inputs import ImageFormat
from pyimgano.models import create_model

detector = create_model(
    "vision_padim",
    pretrained=False,   # avoids weight downloads
    device="cpu",
    image_size=64,      # small demo size
    d_reduced=8,
    projection_fit_samples=1,
    covariance_eps=0.1,
)

train_frames = [np.zeros((64, 64, 3), dtype=np.uint8) for _ in range(8)]
detector.fit(train_frames)
calibrate_threshold(detector, train_frames, input_format=ImageFormat.RGB_U8_HWC, quantile=0.995)

results = infer(
    detector,
    train_frames[:2],
    input_format=ImageFormat.RGB_U8_HWC,
    include_maps=True,
)
print(results[0].score, results[0].label, results[0].anomaly_map.shape)
```

### Example 2: Using Deep Learning (SimpleNet - CVPR 2023)

```python
from pyimgano import models

# State-of-the-art deep learning (CVPR 2023)
# Ultra-fast training - only 10 epochs needed!
detector = models.create_model(
    "vision_simplenet",
    epochs=10,        # 10x faster than other DL methods
    batch_size=8,
    device='cuda'     # GPU acceleration
)

detector.fit(train_paths)
scores = detector.decision_function(test_paths)

# If you need binary labels, threshold the scores.
# For example (one-class, no labels): choose a quantile threshold from train scores.
# import numpy as np
# train_scores = detector.decision_function(train_paths)
# threshold = np.quantile(train_scores, 0.9)  # 0.9 == 1 - contamination (example)
# predictions = (scores >= threshold).astype(int)
```

### Example 3: Maximum Accuracy (PatchCore - CVPR 2022)

```python
# Best accuracy on MVTec AD benchmark (99.6% AUROC)
detector = models.create_model(
    "vision_patchcore",
    backbone='wide_resnet50',
    coreset_sampling_ratio=0.1,  # Memory-efficient
    device='cuda'
)

detector.fit(train_paths)
scores = detector.decision_function(test_paths)

# Get pixel-level anomaly heatmap
anomaly_map = detector.get_anomaly_map('test_image.jpg')
```

### Example 4: Pixel-First Industrial Benchmarking (MVTec AD / VisA) â­ NEW

Run image-level + pixel-level metrics in one command:

```bash
pyimgano-benchmark \
  --dataset mvtec \
  --root /path/to/mvtec_ad \
  --category bottle \
  --model vision_softpatch \
  --device cuda \
  --pixel \
  --pixel-aupro-limit 0.3 \
  --pixel-aupro-thresholds 200 \
  --pixel-postprocess \
  --pixel-post-norm percentile \
  --pixel-post-percentiles 1 99 \
  --output runs/mvtec_bottle_softpatch.json
```

Notes:
- Swap `--model` to compare: `vision_patchcore`, `vision_anomalydino`, `vision_openclip_patchknn`, `vision_openclip_promptscore`.
- For â€œnoisy normalâ€ training sets, `vision_softpatch` is a robust patch-memory baseline.
- Tune AUPRO computation via `--pixel-aupro-limit` (FPR limit, commonly `0.3`) and `--pixel-aupro-thresholds` (integration resolution).
- For deploy-style **single-threshold** pixel evaluation (VAND-style), add:
  - `--pixel-segf1 --pixel-threshold-strategy normal_pixel_quantile --pixel-normal-quantile 0.999`
  - This calibrates one pixel threshold from train/good normal pixels and reports `pixel_segf1` + `bg_fpr`.
- If you train via anomalib, `pyimgano` also provides inference wrappers such as `vision_dinomaly_anomalib` and `vision_cfa_anomalib` (requires `pyimgano[anomalib]` + a trained checkpoint).

Preset tip (popular industrial defaults, no JSON):

```bash
pyimgano-benchmark \
  --dataset mvtec \
  --root /path/to/mvtec_ad \
  --category bottle \
  --model vision_patchcore \
  --preset industrial-balanced \
  --device cuda \
  --pixel
```

Notes:
- Presets:
  - `--preset industrial-fast`: speed-oriented defaults (quick iteration / first run).
  - `--preset industrial-balanced`: speed/accuracy balanced defaults.
  - `--preset industrial-accurate`: accuracy-oriented defaults (higher compute by default).
- Preset model coverage currently includes: `vision_patchcore`, `vision_padim`, `vision_spade`, `vision_anomalydino`, `vision_softpatch`, `vision_simplenet`, `vision_fastflow`, `vision_cflow`, `vision_stfpm`, `vision_reverse_distillation` (alias: `vision_reverse_dist`), `vision_draem`.
- `--model-kwargs` always overrides preset values when both are provided.

Discover available models:

```bash
pyimgano-benchmark --list-models
pyimgano-benchmark --model-info vision_patchcore
pyimgano-benchmark --model-info vision_patchcore --json
```

Benchmarking anomalib-trained checkpoints (inference wrappers):

```bash
# One-time install (keeps anomalib optional):
pip install "pyimgano[anomalib]"

# Evaluate an anomalib checkpoint with pyimgano's unified CLI + reporting.
pyimgano-benchmark \
  --dataset mvtec \
  --root /path/to/mvtec_ad \
  --category bottle \
  --model vision_patchcore_anomalib \
  --checkpoint-path /path/to/anomalib/checkpoint.ckpt \
  --device cuda \
  --pixel \
  --output runs/mvtec_bottle_patchcore_anomalib.json
```

Advanced:
- Pass additional constructor args with `--model-kwargs '{"contamination": 0.1}'`.
- `--checkpoint-path` and `--model-kwargs '{"checkpoint_path": "..."}'` must match (conflicts error out).

### Example 4.5: One-Click Benchmark + Run Artifacts (category=all) â­ NEW

For â€œindustrial-styleâ€ workflows (calibrated threshold + per-image JSONL), run:

```bash
pyimgano-benchmark \
  --dataset mvtec \
  --root /path/to/mvtec_ad \
  --category all \
  --model vision_patchcore \
  --preset industrial-balanced \
  --device cuda
```

By default this writes a run directory under `runs/`:

```
runs/<ts>_<dataset>_<model>/
  report.json
  config.json
  categories/<cat>/report.json
  categories/<cat>/per_image.jsonl
```

Useful flags:
- `--output-dir /path/to/run_dir`: choose where artifacts go
- `--no-save-run`: disable artifact writing (stdout JSON only)
- `--no-per-image-jsonl`: skip per-image records
- `--calibration-quantile 0.995`: override train-calibrated score threshold quantile
- `--limit-train 50 --limit-test 50`: quick smoke runs

### Robustness Benchmark (Clean + Drift Corruptions) â­ NEW

Evaluate a detector on clean test data and a deterministic corruption suite (lighting/JPEG/blur/glare/geo-jitter)
using a **single fixed pixel threshold** for the entire run:

```bash
pyimgano-robust-benchmark \
  --dataset mvtec \
  --root /path/to/mvtec_ad \
  --category bottle \
  --model vision_patchcore \
  --preset industrial-balanced \
  --device cuda \
  --pixel-normal-quantile 0.999 \
  --corruptions lighting,jpeg,blur,glare,geo_jitter \
  --severities 1 2 3 4 5 \
  --output runs/robust_mvtec_bottle_patchcore.json
```

Notes:
- Corruptions require `--input-mode numpy` (default), which feeds detectors **RGB uint8 numpy images**.
- For models that only accept file paths (many classical baselines), use `--input-mode paths`:
  - clean-only evaluation (corruptions are skipped)
  - pixel SegF1 is auto-disabled if the detector does not expose `get_anomaly_map()` / `predict_anomaly_map()`

Docs: `docs/ROBUSTNESS_BENCHMARK.md`

### High-Resolution Tiling Inference (2K/4K) â­ NEW

Many industrial images are much larger than typical model input sizes. `pyimgano` supports tiled inference
to improve tiny-defect sensitivity without resizing the entire frame aggressively.

CLI:

```bash
pyimgano-infer \
  --model vision_patchcore \
  --device cuda \
  --train-dir /path/to/normal/train_images \
  --calibration-quantile 0.995 \
  --input /path/to/high_res_images \
  --include-maps \
  --tile-size 512 \
  --tile-stride 384
```

Python:

```python
from pyimgano.models import create_model
from pyimgano.inference import infer, TiledDetector

det = create_model("vision_patchcore", device="cuda")
det.fit(train_imgs_np, input_format="rgb_u8_hwc")

tiled = TiledDetector(detector=det, tile_size=512, stride=384)
results = infer(tiled, [img_np], input_format="rgb_u8_hwc", include_maps=True)
```

### Example 4: Zero-Shot Detection (WinCLIP - CVPR 2023) â­ NEW

```python
# No training needed! Perfect for quick prototyping
detector = models.create_model(
    "vision_winclip",
    clip_model="ViT-B/32",
    k_shot=0  # Zero-shot mode
)

# Just set the class name and predict
detector.set_class_name("screw")
scores = detector.decision_function(test_paths)
anomaly_maps = detector.predict_anomaly_map(test_paths)  # Pixel-level heatmaps
```

### OpenCLIP Backends (Optional) â­ NEW

```python
# Requires:
#   pip install "pyimgano[clip]"
#
# Notes:
# - OpenCLIP weights are cached by torch (default: ~/.cache/torch).
# - You can override the cache location via TORCH_HOME.
detector = models.create_model(
    "vision_openclip_promptscore",
    device="cuda",  # or "cpu"
    contamination=0.1,
    class_name="screw",
    openclip_model_name="ViT-B-32",
    openclip_pretrained="laion2b_s34b_b79k",
)

detector.fit(train_paths)  # calibrates a threshold from train scores
scores = detector.decision_function(test_paths)
anomaly_map = detector.get_anomaly_map(test_paths[0])
```

### Example 5: Few-Shot Foundation Model (AnomalyDINO - WACV 2025) â­ NEW

```python
from pyimgano.models import create_model

detector = create_model(
    "vision_anomalydino",
    device="cuda",        # optional
    contamination=0.1,
)

# Fit on normal/reference images (builds a patch memory bank)
detector.fit(train_paths)

scores = detector.decision_function(test_paths)
anomaly_map = detector.get_anomaly_map(test_paths[0])
```

> **Note:** The default embedder uses `torch.hub` to load DINOv2 weights on first run.
> For offline/enterprise usage, pass a custom `embedder=...`.

### Example 6: Inference-Only Checkpoint Loading (anomalib backend) â­ NEW

```python
from pyimgano.models import create_model

detector = create_model(
    "vision_anomalib_checkpoint",  # or: vision_patchcore_anomalib, vision_padim_anomalib, ...
    checkpoint_path="/path/to/anomalib.ckpt",
    device="cuda",
    contamination=0.1,
)

# This does not train: it only calibrates a score threshold from train scores.
detector.fit(train_paths)

scores = detector.decision_function(test_paths)
anomaly_map = detector.get_anomaly_map(test_paths[0])
```

### Example 5: Best Localization (SPADE - ECCV 2020) â­ NEW

```python
# Excellent pixel-level anomaly localization
detector = models.create_model(
    "vision_spade",
    backbone="wide_resnet50",
    k_neighbors=50,
    feature_levels=["layer1", "layer2", "layer3"]
)

detector.fit(train_paths)
scores = detector.decision_function(test_paths)
anomaly_maps = detector.predict_anomaly_map(test_paths)  # Precise localization
```

### Example 6: Self-Supervised Learning (CutPaste - CVPR 2021) â­ NEW

```python
# Train without any anomaly samples
detector = models.create_model(
    "vision_cutpaste",
    backbone="resnet18",
    augment_type="3way",  # normal, cutpaste, scar
    epochs=256
)

detector.fit(normal_images_only)  # Only normal images needed
scores = detector.decision_function(test_paths)
```

### Example 7: Comparing Multiple Algorithms

```python
import numpy as np

algorithms = ["vision_ecod", "vision_copod", "vision_simplenet", "vision_spade"]
results = {}

for algo_name in algorithms:
    detector = models.create_model(
        algo_name,
        contamination=0.1
    )
    detector.fit(train_paths)
    train_scores = detector.decision_function(train_paths)
    threshold = np.quantile(train_scores, 0.9)
    test_scores = detector.decision_function(test_paths)
    results[algo_name] = (test_scores >= threshold).astype(int)

# Compare results
for name, preds in results.items():
    print(f"{name}: {preds.sum()} anomalies detected")
```

---

## ğŸ–¼ï¸ Image Preprocessing

Comprehensive preprocessing module with **80+ operations** including augmentation for enhanced anomaly detection:

### Quick Example

```python
from pyimgano.preprocessing import AdvancedImageEnhancer, PreprocessingMixin
from pyimgano.models import ECOD

# Method 1: Using Advanced Enhancer
enhancer = AdvancedImageEnhancer()

# Basic operations
blurred = enhancer.gaussian_blur(image, ksize=(5, 5))
edges = enhancer.detect_edges(blurred, method='canny')

# Advanced operations
retinex = enhancer.retinex_multi(image, sigmas=[15, 80, 250])  # Illumination invariant
lbp = enhancer.compute_lbp(image, n_points=8, radius=1.0)  # Texture features
hog_features = enhancer.extract_hog(image, visualize=False)  # HOG features

# Method 2: Using Mixin with Detector
class ECODWithPreprocessing(PreprocessingMixin, ECOD):
    def __init__(self):
        super().__init__()
        self.setup_preprocessing(enable=True, use_pipeline=True)
        self.add_preprocessing_step('gaussian_blur', ksize=(5, 5))
        self.add_preprocessing_step('normalize', method='minmax')

    def fit(self, X, y=None):
        X_processed = self.preprocess_images(X)
        return super().fit([img.flatten() for img in X_processed], y)

detector = ECODWithPreprocessing()
detector.fit(train_images)
scores = detector.decision_function(test_images)

# Method 3: Data Augmentation for Training
from pyimgano.preprocessing import (
    Compose, OneOf, RandomRotate, RandomFlip,
    ColorJitter, GaussianNoise, get_medium_augmentation
)

# Custom augmentation pipeline
aug_pipeline = Compose([
    RandomFlip(mode="horizontal", p=0.5),
    RandomRotate(angle_range=(-20, 20), p=0.5),
    ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), p=0.5),
    OneOf([
        GaussianNoise(std_range=(10, 25), p=1.0),
        MotionBlur(kernel_size_range=(5, 15), p=1.0),
    ], p=0.3),
])

# Or use preset augmentation
aug_pipeline = get_medium_augmentation()

# Augment training data
augmented_images = [aug_pipeline(img) for img in train_images]
```

### Available Operations (80+)

#### Basic Operations (25)
| Category | Operations | Count |
|----------|------------|-------|
| **Edge Detection** | Canny, Sobel, Laplacian, Scharr, Prewitt, Sobel X/Y | 7 |
| **Morphology** | Erosion, Dilation, Opening, Closing, Gradient, TopHat, BlackHat | 7 |
| **Filters** | Gaussian, Bilateral, Median, Box | 4 |
| **Normalization** | MinMax, Z-Score, L2, Robust | 4 |
| **Enhancement** | Sharpen, Unsharp Mask, CLAHE | 3 |

#### Advanced Operations (25)
| Category | Operations | Count |
|----------|------------|-------|
| **Frequency Domain** | FFT, IFFT, Lowpass, Highpass, Bandpass, Bandstop | 6 |
| **Texture Analysis** | Gabor filters, LBP, GLCM features | 3 |
| **Color Space** | RGB, HSV, LAB, YCrCb, HLS, LUV + conversions | 8 |
| **Enhancement** | Gamma correction, Contrast stretching, Retinex (SSR, MSR) | 4 |
| **Denoising** | Non-local means, Anisotropic diffusion | 2 |
| **Feature Extraction** | HOG, Harris corners, Shi-Tomasi, FAST | 4 |
| **Adv. Morphology** | Skeleton, Thinning, Convex hull, Distance transform | 4 |
| **Segmentation** | Otsu, Adaptive, Triangle, Yen, Watershed | 6 |
| **Pyramids** | Gaussian pyramid, Laplacian pyramid | 2 |

#### Augmentation Operations (30+) â­ NEW!
| Category | Operations | Count |
|----------|------------|-------|
| **Geometric** | Rotate, Flip, Scale, Translate, Shear, Perspective, Affine | 7 |
| **Color** | Brightness, Contrast, Saturation, Hue, Color jitter | 5 |
| **Noise** | Gaussian, Salt-and-pepper, Poisson, Speckle | 4 |
| **Blur** | Motion blur, Defocus blur, Glass blur, Zoom blur | 4 |
| **Weather** | Rain, Fog, Snow, Shadow, Sunflare | 5 |
| **Occlusion** | Random cutout, Grid mask, Mixup, CutMix | 4 |
| **Distortion** | Elastic transform, Grid distortion | 2 |
| **Pipelines** | Compose, OneOf, RandomApply, 5 preset pipelines | 8 |

**See [Preprocessing Guide](docs/PREPROCESSING.md) for detailed usage and best practices**

---

## ğŸ“š Available Algorithms

### Classical Machine Learning (19 algorithms)

| Algorithm | Model Name | Key Features |
|-----------|------------|--------------|
| **ECOD** â­ | `vision_ecod` | Parameter-free, top performance, TKDE 2022 |
| **COPOD** â­ | `vision_copod` | Very fast, parameter-free, ICDM 2020 |
| **Feature Bagging** â­ | `vision_feature_bagging` | Ensemble, high stability |
| KNN | `vision_knn` | Simple, interpretable |
| PCA | `vision_pca` | Classic dimensionality reduction |
| Isolation Forest | `vision_iforest` | Robust, widely used |
| INNE | `vision_inne` | Fast isolation-based |
| LOF | `vision_lof` | Density-based, local outliers |
| COF | `vision_cof` | Connectivity-based |
| MCD | `vision_mcd` | Robust covariance |
| One-Class SVM | `vision_ocsvm` | Kernel methods |
| ABOD | `vision_abod` | Angle-based detection |
| CBLOF | `vision_cblof` | Cluster-based |
| HBOS | `vision_hbos` | Histogram-based |
| KPCA | `vision_kpca` | Kernel PCA |
| LODA | `vision_loda` | Lightweight online |
| LOCI | `vision_loci` | Local correlation |
| SUOD | `vision_suod` | Scalable ensemble |
| XGBOD | `vision_xgbod` | XGBoost-based |

### Deep Learning (33+ algorithms) ğŸ‰

| Algorithm | Model Name | Key Features |
|-----------|------------|--------------|
| **InTra** â­ ğŸ†• | `vision_intra` | Industrial Transformer (ICCV 2023), self-attention |
| **WinCLIP** â­ | `vision_winclip` | Zero-shot CLIP-based (CVPR 2023), no training |
| **OpenCLIP PromptScore** â­ | `vision_openclip_promptscore` | Prompt scoring + anomaly maps (requires `pyimgano[clip]`) |
| **OpenCLIP PatchKNN** â­ | `vision_openclip_patchknn` | OpenCLIP patch embeddings + kNN (requires `pyimgano[clip]`) |
| **SimpleNet** â­ | `vision_simplenet` | Ultra-fast SOTA (CVPR 2023), 10x faster training |
| **BGAD** â­ ğŸ†• | `vision_bgad` | Background-guided (CVPR 2023), robust to variations |
| **DifferNet** â­ | `vision_differnet` | Learnable differences (WACV 2023), k-NN |
| **DSR** â­ | `vision_dsr` | Deep spectral residual (WACV 2023), frequency domain |
| **PatchCore** â­ | `vision_patchcore` | Best accuracy (CVPR 2022), pixel localization |
| **PNI** â­ | `vision_pni` | Pyramidal normality indexing (CVPR 2022), multi-scale |
| **RD++** â­ ğŸ†• | `vision_rdplusplus` | Reverse Distillation++ (2022), enhanced attention |
| **SPADE** â­ | `vision_spade` | Deep pyramid k-NN (ECCV 2020), excellent localization |
| **CutPaste** â­ | `vision_cutpaste` | Self-supervised (CVPR 2021), no anomaly data |
| **DRAEM** â­ | `vision_draem` | Synthetic anomalies (ICCV 2021), robust |
| **CSFlow** | `vision_csflow` | Cross-scale normalizing flows (WACV 2022), expressive |
| **MemSeg** | `vision_memseg` | Memory-guided segmentation (2022) |
| **RIAD** | `vision_riad` | Inpainting-based (2020), self-supervised |
| **DevNet** | `vision_devnet` | Weakly-supervised (KDD 2019), few labels |
| **CFlow-AD** â­ | `vision_cflow` | Conditional flows (WACV 2022), real-time |
| **DFM** â­ | `vision_dfm` | Fast discriminative features, training-free |
| **STFPM** | `vision_stfpm` | Student-Teacher (BMVC 2021), multi-scale |
| FastFlow | `vision_fastflow` | Normalizing flows, real-time |
| PaDiM | `vision_padim` | Patch distribution, edge devices |
| Deep SVDD | `vision_deep_svdd` | One-class deep learning |
| VAE | `vision_vae` | Variational autoencoder |
| AutoEncoder | `vision_ae` | Classic reconstruction |
| Reverse Distillation | `vision_reverse_dist` | Knowledge distillation |
| EfficientAD | `vision_efficientad` | Lightweight, resource-efficient |
| SSIM-based | `vision_ssim` | Structural similarity |
| ALAD | `vision_alad` | Adversarial learning |
| AE+SVM | `vision_ae1svm` | Hybrid approach |
| MO_GAAL | `vision_mo_gaal` | Multi-objective GAN |
| IMDD | `vision_imdd` | Iterative methods |

â­ = Recommended for production use

---

## ğŸ¯ Use Cases

### Industrial Quality Control
```python
# Detect defects in manufactured products
detector = models.create_model("vision_ecod", ...)
detector.fit(normal_product_images)
defect_scores = detector.decision_function(inspection_images)
```

### Medical Imaging
```python
# Identify abnormal X-rays
detector = models.create_model("vision_deep_svdd", ...)
detector.fit(normal_xrays)
abnormal_scores = detector.decision_function(patient_xrays)
```

### Security & Surveillance
```python
# Detect unusual behavior
detector = models.create_model("vision_copod", ...)
detector.fit(normal_scene_frames)
anomaly_scores = detector.decision_function(monitoring_frames)
```

---

## ğŸ“Š Algorithm Comparison & Selection

### Quick Selection Guide

| Your Need | Recommended Algorithms | Why |
|-----------|------------------------|-----|
| **Best Overall Accuracy** | PatchCore, SPADE, FastFlow | 99%+ AUROC on MVTec AD |
| **Fastest Training** | SimpleNet, ECOD, COPOD | 10Ã— faster than competitors |
| **Zero-Shot (No Training)** | WinCLIP | CLIP-based, text prompts |
| **Best Localization** | SPADE, PatchCore, STFPM | Pixel-perfect anomaly maps |
| **Limited Data** | WinCLIP, CutPaste, RIAD | Zero-shot or self-supervised |
| **Real-Time Inference** | SimpleNet, FastFlow, COPOD | 100+ FPS on GPU |
| **No GPU Available** | ECOD, COPOD, Feature Bagging | CPU-optimized classical ML |
| **Few Anomaly Labels** | DevNet | Weakly-supervised learning |
| **Production Deployment** | SimpleNet, ECOD, PatchCore | Stable, well-tested, fast |

### Performance Comparison (MVTec AD Dataset)

| Algorithm | Image AUROC | Pixel AUROC | Training Time | Inference Speed | Memory |
|-----------|-------------|-------------|---------------|-----------------|---------|
| **PatchCore** | 99.6% â­ | 98.7% â­ | Medium | 30-50 FPS | High |
| **SPADE** | 98.0% | 99.0% â­ | Low | 40-60 FPS | Medium |
| **SimpleNet** | 99.0% | 98.0% | Very Low â­ | 100+ FPS â­ | Low |
| **FastFlow** | 99.0% | 98.0% | Low | 60-80 FPS | Medium |
| **DifferNet** | 97.0% | 97.0% | Medium | 20-40 FPS | Medium |
| **CutPaste** | 96.0% | N/A | Medium | 50+ FPS | Low |
| **STFPM** | 97.0% | 98.0% | Medium | 40-60 FPS | Medium |
| **WinCLIP** | 95.0% | 98.0% | None â­ | 5-10 FPS | Low |
| **ECOD** | 85-90% | N/A | None â­ | 200+ FPS â­ | Very Low â­ |
| **COPOD** | 85-90% | N/A | None â­ | 300+ FPS â­ | Very Low â­ |

### Decision Tree

```
Start Here
â”‚
â”œâ”€ Have GPU?
â”‚  â”œâ”€ YES
â”‚  â”‚  â”œâ”€ Need best accuracy? â†’ PatchCore, SPADE
â”‚  â”‚  â”œâ”€ Need speed? â†’ SimpleNet, FastFlow
â”‚  â”‚  â”œâ”€ No training data? â†’ WinCLIP (zero-shot)
â”‚  â”‚  â””â”€ Need localization? â†’ SPADE, PatchCore
â”‚  â”‚
â”‚  â””â”€ NO (CPU only)
â”‚     â”œâ”€ Need speed? â†’ COPOD, ECOD
â”‚     â”œâ”€ Need accuracy? â†’ ECOD, Feature Bagging
â”‚     â””â”€ General purpose â†’ ECOD (best balance)
â”‚
â”œâ”€ Type of Data?
â”‚  â”œâ”€ Only normal samples â†’ CutPaste, RIAD, WinCLIP
â”‚  â”œâ”€ Few anomaly labels â†’ DevNet
â”‚  â””â”€ Mixed normal/anomaly â†’ Any algorithm
â”‚
â””â”€ Deployment Scenario?
   â”œâ”€ Edge device â†’ PaDiM, EfficientAD
   â”œâ”€ Cloud/Server â†’ PatchCore, SimpleNet
   â””â”€ Real-time critical â†’ SimpleNet, COPOD
```

---

## ğŸ› ï¸ Utility Functions

PyImgAno includes comprehensive utility functions for the complete anomaly detection workflow:

### Dataset Loading ğŸ“Š
```python
from pyimgano.utils import MVTecDataset, load_dataset

# Load MVTec AD dataset
dataset = MVTecDataset(root='./mvtec_ad', category='bottle', resize=(256, 256))
train_data = dataset.get_train_data()
test_data, test_labels, test_masks = dataset.get_test_data()

# Or use factory function
dataset = load_dataset('mvtec', './mvtec_ad', category='bottle')
```

**Supported Datasets:**
- MVTec AD (15 categories) â€” `load_dataset("mvtec", ...)`
- MVTec LOCO AD (5 categories) â€” `load_dataset("mvtec_loco", ...)`
- MVTec AD 2 (2025; `test_public` split) â€” `load_dataset("mvtec_ad2", ...)`
- VisA â€” `load_dataset("visa", ...)`
- BTAD (3 categories) â€” `load_dataset("btad", ...)`
- Custom datasets (flexible structure) â€” `load_dataset("custom", ...)`

### Advanced Visualization ğŸ“ˆ
```python
from pyimgano.utils import (
    plot_roc_curve, plot_confusion_matrix,
    plot_score_distribution, create_evaluation_report
)

# ROC curve
auc_score, fig = plot_roc_curve(y_true, y_scores, save_path='roc.png')

# Confusion matrix
plot_confusion_matrix(y_true, y_pred, labels=['Normal', 'Anomaly'])

# Score distribution
plot_score_distribution(normal_scores, anomaly_scores)

# Complete evaluation report
figures = create_evaluation_report(y_true, y_scores, y_pred, model_name='PatchCore')
```

**Available Plots:**
- ROC curves
- Precision-Recall curves
- Confusion matrices
- Score distributions
- t-SNE feature space visualization
- Anomaly heatmaps
- Multi-model comparisons
- Threshold analysis

### Model Management ğŸ’¾
```python
from pyimgano.utils import save_model, load_model, ModelRegistry, profile_model

# Save/load models
save_model(detector, 'model.pkl', metadata={'auc': 0.98})
detector = load_model('model.pkl')

# Model registry
registry = ModelRegistry('./models')
registry.register('patchcore_v1', detector, metadata={'version': '1.0'})
model = registry.load('patchcore_v1')

# Profile performance
metrics = profile_model(detector, test_data, n_runs=10)
print(f"Avg time: {metrics['avg_time_ms']:.2f} ms")
```

**Features:**
- Save/load with metadata
- Model registry for version management
- Checkpointing
- Performance profiling
- Model comparison
- Configuration export

### Experiment Tracking ğŸ”¬
```python
from pyimgano.utils import ExperimentTracker, track_experiment

# Create tracker
tracker = ExperimentTracker('./experiments')

# Create experiment
exp = tracker.create_experiment('patchcore_bottle', model_type='PatchCore')
exp.log_params({'backbone': 'resnet50', 'lr': 0.001})
exp.log_metric('auc', 0.98)
exp.add_tag('production')

# Quick experiment tracking
exp = track_experiment(
    'my_experiment',
    model=detector,
    train_data=train_imgs,
    test_data=test_imgs,
    test_labels=test_labels,
    backbone='resnet50'
)

# Generate report
report = tracker.generate_report(exp_id, output_path='report.md')
```

**Features:**
- Hyperparameter logging
- Metric tracking over time
- Artifact management
- Experiment comparison
- Markdown report generation

---

## ğŸ“– Documentation

### Core Guides
- **[Quick Start Guide](docs/QUICK_START.md)** â­ - Get started in 5 minutes
- **[SOTA Algorithms Guide](docs/SOTA_ALGORITHMS.md)** â­ - Latest state-of-the-art algorithms (WinCLIP, SPADE, etc.)
- **[Deep Learning Models Guide](docs/DEEP_LEARNING_MODELS.md)** â­ - Comprehensive deep learning guide
- **[Algorithm Selection Guide](docs/ALGORITHM_SELECTION_GUIDE.md)** - Choose the right algorithm
- **[Preprocessing Guide](docs/PREPROCESSING.md)** â­ - Image enhancement and preprocessing
- **[Evaluation & Benchmarking Guide](docs/EVALUATION_AND_BENCHMARK.md)** â­ - Comprehensive evaluation tools
- **[Utilities Guide](examples/utilities_example.py)** â­ NEW! - Dataset loading, visualization, model management

### Reference
- **[API Reference](docs/)** - Detailed API documentation
- **[Examples](examples/)** - Code examples and tutorials
- **[Contributing](CONTRIBUTING.md)** - Contribution guidelines
- **[Changelog](CHANGELOG.md)** - Version history

---

## ğŸ—ï¸ Project Structure

```
pyimgano/
â”œâ”€â”€ pyimgano/
â”‚   â”œâ”€â”€ models/          # 50+ anomaly detection algorithms ğŸ‰
â”‚   â”‚   â”œâ”€â”€ Classical ML (19 algorithms)
â”‚   â”‚   â”‚   â”œâ”€â”€ ecod.py          # ECOD (TKDE 2022)
â”‚   â”‚   â”‚   â”œâ”€â”€ copod.py         # COPOD (ICDM 2020)
â”‚   â”‚   â”‚   â”œâ”€â”€ feature_bagging.py
â”‚   â”‚   â”‚   â”œâ”€â”€ knn.py, pca.py, lof.py, ...
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ Deep Learning (31 algorithms) ğŸ‰
â”‚   â”‚   â”‚   â”œâ”€â”€ intra.py         # InTra Transformer (ICCV 2023) â­ ğŸ†•
â”‚   â”‚   â”‚   â”œâ”€â”€ winclip.py       # WinCLIP (CVPR 2023) â­
â”‚   â”‚   â”‚   â”œâ”€â”€ simplenet.py     # SimpleNet (CVPR 2023) â­
â”‚   â”‚   â”‚   â”œâ”€â”€ bgad.py          # BGAD (CVPR 2023) â­ ğŸ†•
â”‚   â”‚   â”‚   â”œâ”€â”€ differnet.py     # DifferNet (WACV 2023) â­
â”‚   â”‚   â”‚   â”œâ”€â”€ dsr.py           # DSR (WACV 2023) â­ ğŸ†•
â”‚   â”‚   â”‚   â”œâ”€â”€ patchcore.py     # PatchCore (CVPR 2022) â­
â”‚   â”‚   â”‚   â”œâ”€â”€ pni.py           # PNI (CVPR 2022) â­ ğŸ†•
â”‚   â”‚   â”‚   â”œâ”€â”€ rdplusplus.py    # RD++ (2022) â­ ğŸ†•
â”‚   â”‚   â”‚   â”œâ”€â”€ stfpm.py         # STFPM (BMVC 2021) â­
â”‚   â”‚   â”‚   â”œâ”€â”€ cutpaste.py      # CutPaste (CVPR 2021) â­
â”‚   â”‚   â”‚   â”œâ”€â”€ draem.py         # DRAEM (ICCV 2021) â­
â”‚   â”‚   â”‚   â”œâ”€â”€ cflow.py         # CFlow-AD (WACV 2022) â­
â”‚   â”‚   â”‚   â”œâ”€â”€ csflow.py        # CSFlow (WACV 2022) ğŸ†•
â”‚   â”‚   â”‚   â”œâ”€â”€ spade.py         # SPADE (ECCV 2020) â­
â”‚   â”‚   â”‚   â”œâ”€â”€ memseg.py        # MemSeg (2022)
â”‚   â”‚   â”‚   â”œâ”€â”€ riad.py          # RIAD (2020)
â”‚   â”‚   â”‚   â”œâ”€â”€ devnet.py        # DevNet (KDD 2019)
â”‚   â”‚   â”‚   â”œâ”€â”€ dfm.py           # DFM (training-free) â­
â”‚   â”‚   â”‚   â”œâ”€â”€ fastflow.py, padim.py, ...
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ registry.py  # Model registry system
â”‚   â”‚   â””â”€â”€ baseml.py    # Base classes
â”‚   â”œâ”€â”€ preprocessing/   # Image preprocessing module â­ NEW!
â”‚   â”‚   â”œâ”€â”€ enhancer.py  # 20+ enhancement operations
â”‚   â”‚   â”œâ”€â”€ mixin.py     # Easy integration mixin
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ utils/           # Image processing utilities
â”‚   â”œâ”€â”€ datasets/        # Data loading utilities
â”‚   â”œâ”€â”€ evaluation.py    # Evaluation metrics â­
â”‚   â”œâ”€â”€ benchmark.py     # Algorithm benchmarking â­
â”‚   â””â”€â”€ visualization/   # Visualization tools
â”œâ”€â”€ tests/               # Comprehensive test suite
â”‚   â”œâ”€â”€ test_pyod_models.py      # Classical ML tests
â”‚   â”œâ”€â”€ test_dl_models.py        # Deep learning tests
â”‚   â”œâ”€â”€ test_preprocessing.py    # Preprocessing tests â­ NEW!
â”‚   â”œâ”€â”€ test_evaluation.py       # Evaluation tests
â”‚   â””â”€â”€ ...
â”œâ”€â”€ examples/            # Usage examples
â”‚   â”œâ”€â”€ preprocessing_example.py # Preprocessing guide â­ NEW!
â”‚   â”œâ”€â”€ benchmark_example.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ docs/                # Documentation
â”‚   â”œâ”€â”€ DEEP_LEARNING_MODELS.md  # DL algorithms guide â­
â”‚   â”œâ”€â”€ PREPROCESSING.md         # Preprocessing guide â­ NEW!
â”‚   â”œâ”€â”€ EVALUATION_AND_BENCHMARK.md
â”‚   â”œâ”€â”€ ALGORITHM_SELECTION_GUIDE.md
â”‚   â””â”€â”€ ...
â””â”€â”€ .github/             # CI/CD workflows
```

---

## ğŸ§ª Testing

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=pyimgano --cov-report=html

# Run specific tests
pytest tests/test_pyod_models.py -v

# Run with multiple Python versions
tox
```

---

## ğŸ› ï¸ Development

### Setup Development Environment

```bash
# Clone repository
git clone https://github.com/jhlu2019/pyimgano.git
cd pyimgano

# Install in development mode
pip install -e .[dev]

# Install pre-commit hooks
pre-commit install
```

### Code Quality

We maintain high code quality standards:

```bash
# Format code
make format

# Run linters
make lint

# Type checking
make type

# Run all checks
make all
```

### Contributing

We welcome contributions! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

---

## ğŸ“Š Benchmarks

Based on [ADBench](https://github.com/Minqi824/ADBench) (30 algorithms on 57 datasets):

| Algorithm | Average Rank | Relative Speed |
|-----------|--------------|----------------|
| ECOD | 3.5/30 | 1.2Ã— |
| COPOD | 4.2/30 | 1.0Ã— (fastest) |
| IForest | 5.8/30 | 2.0Ã— |
| LODA | 6.1/30 | 1.8Ã— |
| Deep SVDD | 8.5/30 | 10Ã— |

---

## ğŸ”— Related Projects

- **[PyOD](https://github.com/yzhao062/pyod)** - Python Outlier Detection Library
- **[Anomalib](https://github.com/openvinotoolkit/anomalib)** - Deep Learning Anomaly Detection
- **[ADBench](https://github.com/Minqi824/ADBench)** - Anomaly Detection Benchmark

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ“® Citation

If you use PyImgAno in your research, please cite:

```bibtex
@software{pyimgano2025,
  author = {PyImgAno Contributors},
  title = {PyImgAno: Enterprise-Grade Visual Anomaly Detection Toolkit},
  year = {2025},
  url = {https://github.com/jhlu2019/pyimgano}
}
```

---

## ğŸ¤ Acknowledgments

- [PyOD](https://github.com/yzhao062/pyod) for the excellent outlier detection library
- [Anomalib](https://github.com/openvinotoolkit/anomalib) for deep learning inspiration
- All contributors and users of PyImgAno

---

## ğŸ“ Contact & Support

- **Issues:** [GitHub Issues](https://github.com/jhlu2019/pyimgano/issues)
- **Discussions:** [GitHub Discussions](https://github.com/jhlu2019/pyimgano/discussions)
- **Documentation:** [Read the Docs](https://pyimgano.readthedocs.io/)

---

**Made with â¤ï¸ by the PyImgAno Team**
